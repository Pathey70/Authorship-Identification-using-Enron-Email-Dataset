{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822ec6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from textstat.textstat import textstatistics,legacy_round\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31273d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    rem_new_line = \"\".join([s for s in rem_num.strip().splitlines(True) if s.strip(\"\\r\\n\").strip()])\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_new_line)  \n",
    "    filtered_words = [w for w in tokens]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_specific_feature(spacy_doc):\n",
    "    sents = list(spacy_doc.sents)\n",
    "    words = []\n",
    "    \n",
    "    num_words = 0\n",
    "    num_chr = 0\n",
    "    num_long_words = 0\n",
    "    num_stop_words = 0\n",
    "    num_not_vocab_word = 0\n",
    "    highest_frequency = 0\n",
    "    \n",
    "    TTR = 0\n",
    "    hapaxes = []\n",
    "    HTR = 0\n",
    "    \n",
    "    for token in spacy_doc:\n",
    "        if token.is_alpha:\n",
    "            num_words+=1\n",
    "            num_chr+=len(str(token))\n",
    "            if len(token) > 5:\n",
    "                num_long_words+=1\n",
    "            if not token.vocab:\n",
    "                num_not_vocab_word+=1\n",
    "            if token.is_stop:\n",
    "                num_stop_words+=1\n",
    "            words.append(str(token))\n",
    "            \n",
    "    avg_num_chr_word = num_chr/num_words if num_words>0 else 0\n",
    "    if words:\n",
    "        TTR = len(set(words))/len(words)\n",
    "        hapaxes = list(filter(lambda x: words.count(x) == 1, words))\n",
    "        HTR = len(hapaxes)/ len(words)\n",
    "        dec_word_frequencies = sorted(Counter(words).items(), key=lambda x:x[1], reverse=True)\n",
    "        highest_frequency = dec_word_frequencies[0][1]\n",
    "        \n",
    "    num_sents = len(sents)\n",
    "    avg_num_word_sen = sum([len(sent) for sent in sents])/num_sents\n",
    "    \n",
    "    return pd.Series({\"num_sents\" : num_sents,\n",
    "                      \"avg_num_word_sen\" : avg_num_word_sen,\n",
    "                      \"num_words\" : num_words,\n",
    "                      \"num_long_words\" : num_long_words,\n",
    "                      \"num_stop_words\" : num_stop_words,\n",
    "                      \"num_not_vocab_word\" : num_not_vocab_word,\n",
    "                      \"num_chr\" : num_chr,\n",
    "                      \"avg_num_chr_word\": avg_num_chr_word,\n",
    "                      \"TTR\" : TTR,\n",
    "                      \"HTR\" : HTR,\n",
    "                      \"highest_frequency\" : highest_frequency\n",
    "                     })\n",
    "\n",
    "def punctuation_specific_feature(spacy_doc):\n",
    "    num_com_email = 0\n",
    "    num_dot_email = 0\n",
    "    num_exc_email = 0\n",
    "    num_que_email = 0\n",
    "    num_col_email = 0\n",
    "    num_semi_col_email = 0\n",
    "    \n",
    "    words = list(map(str,spacy_doc))\n",
    "    for word in words:\n",
    "        if word == ',':\n",
    "            num_com_email+=1\n",
    "        elif word == '.':\n",
    "            num_dot_email+=1\n",
    "        elif word == '!':\n",
    "            num_exc_email+=1\n",
    "        elif word == '?':\n",
    "            num_que_email+=1\n",
    "        elif word == ':':\n",
    "            num_col_email+=1\n",
    "        elif word == ';':\n",
    "            num_semi_col_email+=1\n",
    "    return pd.Series({\"num_com_email\" : num_com_email,\n",
    "                      \"num_dot_email\" : num_dot_email,\n",
    "                      \"num_exc_email\" : num_exc_email,\n",
    "                      \"num_que_email\" : num_que_email,\n",
    "                      \"num_col_email\" : num_col_email,\n",
    "                      \"num_semi_col_email\" : num_semi_col_email\n",
    "                     })\n",
    "\n",
    "def syntactic_specific_feature(spacy_doc):\n",
    "    num_func_words = 0\n",
    "    avg_verb_email = 0\n",
    "    num_pos_email = 0\n",
    "    pos_tags = [token.pos_ for token in spacy_doc]\n",
    "    for tag in pos_tags:\n",
    "        if tag in [\"PRON\", \"DET\", \"ADP\", \"CONJ\", \"AUX\"]:\n",
    "            num_func_words+=1\n",
    "    avg_verb_email = pos_tags.count(\"VERB\")/len(spacy_doc)\n",
    "    num_pos_email = len(set(pos_tags))\n",
    "    return pd.Series({\"num_func_words\" : num_func_words,\n",
    "                      \"avg_verb_email\" : avg_verb_email,\n",
    "                      \"num_pos_email\" : num_pos_email\n",
    "                     }) \n",
    "\n",
    "def semantic_specific_feature(spacy_doc, raw_doc):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarity_scores = sid.polarity_scores(raw_doc)\n",
    "    num_pos_word = 0\n",
    "    num_neg_word = 0\n",
    "    num_named_entity = 0\n",
    "    \n",
    "    for token in spacy_doc:\n",
    "        token_pol_score = sid.polarity_scores(str(token))['compound']\n",
    "        if token_pol_score >= 0.5:\n",
    "            num_pos_word+=1\n",
    "        elif token_pol_score <= -0.5:\n",
    "            num_neg_word+=1\n",
    "        if token.ent_type_ !=\"\":\n",
    "            num_named_entity+=1\n",
    "    return pd.Series({\"num_pos_word\" : num_pos_word,\n",
    "                      \"num_neg_word\" : num_neg_word,\n",
    "                      \"num_named_entity\" : num_named_entity,\n",
    "                      \"polarity_score\" : polarity_scores['compound']\n",
    "                     })\n",
    "\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "\n",
    "def difficult_words(spacy_doc):\n",
    "    sents = list(spacy_doc.sents)\n",
    "    words = []\n",
    "    for sent in sents:\n",
    "        words += [str(token) for token in sent]\n",
    "    \n",
    "    diff_words = set()\n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words.add(word)\n",
    "    return len(diff_words)\n",
    "\n",
    "def dale_chall_index(spacy_doc):\n",
    "    word_count = len(spacy_doc)\n",
    "    avg_sentence_length = word_count/len(list(spacy_doc.sents))\n",
    "    count = word_count - difficult_words(spacy_doc)\n",
    "    if word_count>0:\n",
    "        per = float(count) / float(word_count) * 100\n",
    "    diff_words = 100-per\n",
    "    score = (0.1579 * diff_words) + \\\n",
    "                (0.0496 * avg_sentence_length)\n",
    "    if diff_words > 5:\n",
    "        score += 3.6365\n",
    "    return legacy_round(score,2)\n",
    "\n",
    "\n",
    "def smog_index(spacy_doc):\n",
    "    sent_count = len(list(spacy_doc.sents))\n",
    "    poly_syllable_count = 0\n",
    "    for word in list(map(str,spacy_doc)):\n",
    "        syllable_count = syllables_count(word)\n",
    "        if syllable_count >= 3:\n",
    "            poly_syllable_count+=1\n",
    "    if sent_count >= 3:\n",
    "        SMOG = (1.043 * (30*(poly_syllable_count / sent_count))**0.5) \\\n",
    "                + 3.1291\n",
    "        return legacy_round(SMOG, 1)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def flesch_reading_index(spacy_doc):\n",
    "    avg_sentence_length = len(spacy_doc)/len(list(spacy_doc.sents))\n",
    "    avg_syllables_per_word = sum([syllables_count(word) for word in list(map(str,spacy_doc))])/len(spacy_doc)\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length) -\\\n",
    "          float(84.6 * avg_syllables_per_word)\n",
    "    return legacy_round(FRE, 2)\n",
    "\n",
    "    \n",
    "def readability_specific_features(spacy_doc):\n",
    "    return pd.Series({\n",
    "        \"smog_idx\" : smog_index(spacy_doc),\n",
    "        \"dale_chall_idx\" : dale_chall_index(spacy_doc),\n",
    "        \"flesch_idx\" : flesch_reading_index(spacy_doc)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df = pd.read_csv('kaggle_enron_email_cleaned.csv')\n",
    "filtered_df = emails_df.groupby('sender').filter(lambda g: g.count().gt(2500).any())\n",
    "grp_df = filtered_df.groupby('sender')\n",
    "filtered_author_list = [key for key, item in grp_df]\n",
    "filtered_author_count = [grp_df.get_group(key).count() for key, item in grp_df]\n",
    "filtered_df['content'] = filtered_df['email_body'].map(lambda s:preprocess(s)) \n",
    "filtered_df['sender'] = filtered_df['sender'].astype('category')\n",
    "filtered_df['author'] = filtered_df['sender'].cat.codes\n",
    "filtered_df.drop('email_body', axis=1, inplace=True)\n",
    "filtered_df.drop('sender', axis=1, inplace=True)\n",
    "filtered_df.drop('file', axis=1, inplace=True)\n",
    "filtered_df.drop('valid', axis=1, inplace=True)\n",
    "filtered_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "filtered_df.drop('sender_email', axis=1, inplace=True)\n",
    "filtered_df[\"email_body\"] = filtered_df[\"content\"].apply(lambda x: nlp(x))\n",
    "filtered_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "filtered_df.dropna(subset = [\"content\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.join(filtered_df.email_body.apply(readability_specific_features))\n",
    "filtered_df = filtered_df.join(filtered_df.email_body.apply(syntactic_specific_feature))\n",
    "filtered_df = filtered_df.join(filtered_df.apply(lambda x : semantic_specific_feature(x[\"email_body\"], x[\"content\"]), axis=1))\n",
    "filtered_df = filtered_df.join(filtered_df.email_body.apply(punctuation_specific_feature))\n",
    "filtered_df = filtered_df.join(filtered_df.email_body.apply(content_specific_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a678d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.drop('email_body', axis=1, inplace=True)\n",
    "filtered_df_copy = filtered_df.copy()\n",
    "features = list(filtered_df.columns)\n",
    "features.remove(\"content\")\n",
    "features.remove(\"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scalar = StandardScaler()\n",
    "standardized_features = pd.DataFrame(scalar.fit_transform(filtered_df[features].copy()), columns = features)\n",
    "standardized_features[\"author\"] = filtered_df[\"author\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca268be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardized_features.iloc[:,standardized_features.columns != 'author']\n",
    "y = standardized_features.author\n",
    "pca = PCA()\n",
    "x_new = pca.fit_transform(X)\n",
    "\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "myplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17859a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardized_features.iloc[:,standardized_features.columns != 'author']\n",
    "y = standardized_features.author\n",
    "model = PCA(n_components=15).fit(X)\n",
    "X_pc = model.transform(X)\n",
    "n_pcs= model.components_.shape[0]\n",
    "most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = list(X.columns)\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "print(most_important_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
    "df = pd.DataFrame(dic.items())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aaa565",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features = standardized_features[most_important_names].copy()\n",
    "pca_features = pca_features.loc[:,~pca_features.columns.duplicated()]\n",
    "pca_features[\"content\"] = filtered_df[\"content\"].values\n",
    "pca_features[\"author\"] = filtered_df[\"author\"].values\n",
    "pca_features[\"content\"] = pca_features[\"content\"].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcda5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features_tfidf = pca_features.copy()\n",
    "pca_features_cv = pca_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92550dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_dense = tfidf_vec.fit_transform(pca_features_tfidf['content']).todense()\n",
    "new_cols = tfidf_vec.get_feature_names_out()\n",
    "pca_features_tfidf = pca_features_tfidf.drop('author',axis=1)\n",
    "pca_features_tfidf = pca_features_tfidf.drop('content',axis=1)\n",
    "pca_features_tfidf = pca_features_tfidf.join(pd.DataFrame(tfidf_dense, columns=new_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8946ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tfidf = Pipeline([(\"random forest\",\n",
    "                         RandomForestClassifier(n_estimators=100))   \n",
    "])\n",
    "\n",
    "svc_tfidf = Pipeline([\n",
    "               (\"linear svc\", \n",
    "                SVC(kernel=\"linear\", probability=True))])\n",
    "\n",
    "X = pca_features.iloc[:,pca_features.columns != 'author']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, filtered_df.author,\n",
    "                                                    stratify=filtered_df.author, \n",
    "                                                    test_size=0.20)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print('Training set score: ' + str(rf_tfidf.score(X_train,y_train)))\n",
    "print('Test set score: ' + str(rf_tfidf.score(X_test,y_test)))\n",
    "\n",
    "svc_tfidf.fit(X_train, y_train)\n",
    "print('Training set score: ' + str(svc_tfidf.score(X_train,y_train)))\n",
    "print('Test set score: ' + str(svc_tfidf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b072218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = Pipeline([(\"random forest\",\n",
    "                         RandomForestClassifier(n_estimators=100))   \n",
    "])\n",
    "\n",
    "svc_cv = Pipeline([('vect', CountVectorizer()),\n",
    "               (\"linear svc\", \n",
    "                SVC(kernel=\"linear\", probability=True))])\n",
    "\n",
    "X = pca_features_cv.iloc[:,pca_features_cv.columns != 'author']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, filtered_df.author,\n",
    "                                                    stratify=filtered_df.author, \n",
    "                                                    test_size=0.20)\n",
    "preproc = ColumnTransformer(\n",
    "    [('text_vect', CountVectorizer(), 'content')],\n",
    "    remainder='passthrough',\n",
    ")\n",
    "X_train_preproc = preproc.fit_transform(X_train)\n",
    "X_test_preproc = preproc.transform(X_test)\n",
    "\n",
    "rf_cv.fit(X_train_preproc, y_train)\n",
    "print('Training set score: ' + str(rf_cv.score(X_train_preproc,y_train)))\n",
    "print('Test set score: ' + str(rf_cv.score(X_test_preproc,y_test)))\n",
    "\n",
    "svc_cv.fit(X_train_preproc, y_train)\n",
    "print('Training set score: ' + str(svc_cv.score(X_train_preproc,y_train)))\n",
    "print('Test set score: ' + str(svc_cv.score(X_test_preproc,y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
